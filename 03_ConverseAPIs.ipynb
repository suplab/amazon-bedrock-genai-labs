{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtv8Mg8I7pfPHalAYcL1nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suplab/amazon-bedrock-genai-labs/blob/main/03_ConverseAPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converse APIs"
      ],
      "metadata": {
        "id": "QbY-Ttpl6DVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Amazon Bedrock Converse API enables developers to build sophisticated conversational applications while managing context across multiple interactions. Unlike standard foundation model calls which are stateless, the Converse API provides a structured way to maintain conversation history and context, making it ideal for chatbots, virtual assistants, and other interactive applications.\n",
        "\n",
        "Converse API also provides a unified way to invoke foundation models, making it easier to swap out models when needed."
      ],
      "metadata": {
        "id": "DqL0VB6f6JGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks of the Converse API\n",
        "Now that we understand what the Converse API is and its purpose, let's examine the essential components that make it work. Each component plays a specific role in creating effective conversational experiences, and understanding how they interact is crucial for building robust applications.\n",
        "\n",
        "From system prompts that shape the model's behavior to tool configurations that enable real-world actions, these building blocks can be combined in various ways to create exactly the conversational experience you need.\n",
        "\n",
        "Let's break down each component, explore practical examples, and review best practices for implementation.\n",
        "\n",
        "### System prompt\n",
        "A system prompt defines the model's behavior, personality, and operating parameters. It sets the foundation for how the model should interact and what type of responses it should provide.\n",
        "\n",
        "```\n",
        "system_prompt = \"You are an AI assistant that helps create playlists. Only return song names and artists.\"\n",
        "```\n",
        "\n",
        "### Message Array\n",
        "Message arrays store the conversation history as a sequence of interactions between the user and the assistant. Each message includes a role identifier and the content of the message.\n",
        "\n",
        "```\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Create a playlist of 3 pop songs\"\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "### Inference Parameters\n",
        "Inference parameters control how the model generates responses. Different models support different parameters, so it's important to check the model-specific documentation.Example for Claude models:\n",
        "\n",
        "```\n",
        "inference_config = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"top_p\": 0.999,\n",
        "}\n",
        "additional_model_fields = {\"top_k\": top_k}\n",
        "```\n",
        "\n",
        "### Invoking the Converse API\n",
        "\n",
        "```\n",
        "response = bedrock.converse(\n",
        "    modelId=\"<MODEL_ID_CLAUDE>\",\n",
        "    messages=messages,\n",
        "    system=system_prompt,\n",
        "    inferenceConfig=inference_config,\n",
        "    additionalModelRequestFields=additional_model_fields\n",
        ")\n",
        "```\n",
        "\n",
        "### Tool Configuration\n",
        "Tool configurations enable the model to interact with external functions.Here's an example of defining a weather tool and then passing the tool to the Converse API:\n",
        "\n",
        "```\n",
        "weather_tool = {\n",
        "    \"toolSpec\": {\n",
        "        \"name\": \"getWeather\",\n",
        "        \"description\": \"Gets the current weather using latitude and longitude.\",\n",
        "        \"inputSchema\": {\n",
        "            \"json\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"lat\": {\"type\": \"number\"},\n",
        "                    \"lon\": {\"type\": \"number\"}\n",
        "                },\n",
        "                \"required\": [\"lat\", \"lon\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "initial_response = bedrock.converse(\n",
        "    modelId=\"<MODEL_ID>\",\n",
        "    system=\"<system_prompt>\",\n",
        "    messages=[\"<initial_user_msg>\"],\n",
        "    toolConfig={\n",
        "        \"tools\": [weather_tool],\n",
        "        \"toolChoice\": {\"auto\": {}}\n",
        "    },\n",
        "    inferenceConfig={\"temperature\": 0.7}\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "ZERmHBlc6UR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Bedrock"
      ],
      "metadata": {
        "id": "RSdximLK7QzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4Cw3KUwX5ct3"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "bedrock = boto3.client('bedrock−runtime', region_name='us-east-1')\n",
        "bedrock_agent = boto3.client(service_name='bedrock−agent', region_name='us-east-1')\n",
        "\n",
        "MODEL_ID = \"amazon.nova−micro−v1:0\""
      ],
      "metadata": {
        "id": "apcvVG0A7XoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traveling with Bedrock"
      ],
      "metadata": {
        "id": "H0kTPADF7010"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = .7\n",
        "\n",
        "inference_config = {\"temperature\": temperature}\n",
        "\n",
        "system_prompts = [{\"text\": \"You are a virtual travel assistant that suggests destinations based on user preferences.\"\n",
        "                + \"Only return destination names and a brief description.\"}]\n",
        "\n",
        "messages = []\n",
        "\n",
        "message_1 = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"text\": \"Create a list of 3 travel destinations.\"}]\n",
        "}\n",
        "\n",
        "messages.append(message_1)\n",
        "\n",
        "response = bedrock.converse(\n",
        "    modelId=MODEL_ID,\n",
        "    messages=messages,\n",
        "    system=system_prompts,\n",
        "    inferenceConfig=inference_config\n",
        ")\n",
        "\n",
        "def print_response(response):\n",
        "    model_response = response.get('output', {}).get('message', {}).get('content', [{}])[0].get('text', '')\n",
        "\n",
        "    print(\"✈️ Your suggested travel destinations:\")\n",
        "    print(model_response)\n",
        "\n",
        "print_response(response)"
      ],
      "metadata": {
        "id": "FfcP8LVh7vNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuing the conversation"
      ],
      "metadata": {
        "id": "WJ4h7XDE7VQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_2 = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"text\": \"Only suggest travel locations that are no more than one short flight away.\"}]\n",
        "}\n",
        "\n",
        "messages.append(message_2)\n",
        "\n",
        "response = bedrock.converse(\n",
        "    modelId=MODEL_ID,\n",
        "    messages=messages,\n",
        "    system=system_prompts,\n",
        "    inferenceConfig=inference_config\n",
        ")\n",
        "\n",
        "print_response(response)"
      ],
      "metadata": {
        "id": "U1ZWFonE77aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refine the results"
      ],
      "metadata": {
        "id": "ftvdfdod7-mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_3 = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"text\": \"List destinations by their proximity to Seattle, WA USA\"}]\n",
        "}\n",
        "\n",
        "messages.append(message_3)\n",
        "\n",
        "response = bedrock.converse(\n",
        "    modelId=MODEL_ID,\n",
        "    messages=messages,\n",
        "    system=system_prompts,\n",
        "    inferenceConfig=inference_config\n",
        ")\n",
        "\n",
        "print_response(response)"
      ],
      "metadata": {
        "id": "QVwqDG0R8CcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Bedrock Prompt"
      ],
      "metadata": {
        "id": "N1UiZiOm8PyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response = bedrock_agent.create_prompt(\n",
        "        name=\"Travel-Agent-Prompt\",\n",
        "        description=\"Checks if all trip information has been provided.\",\n",
        "        variants=[\n",
        "            {\n",
        "                \"name\": \"Variant1\",\n",
        "                \"modelId\": MODEL_ID,\n",
        "                \"templateType\": \"CHAT\",\n",
        "                \"inferenceConfiguration\": {\n",
        "                    \"text\": {\n",
        "                        \"temperature\": 0.4\n",
        "                    }\n",
        "                },\n",
        "                \"templateConfiguration\": {\n",
        "                    \"chat\": {\n",
        "                        'system': [\n",
        "                            {\n",
        "                                \"text\": \"\"\"You are a travel agent evaluating trip requests for custom itineraries.\n",
        "                                Review the message carefully and answer YES or NO to the following screening questions.\n",
        "                                Be strict—if any detail is missing or unclear, answer NO.\n",
        "\n",
        "                                A) Is the destination clearly stated?\n",
        "                                B) Are the travel dates within a reasonable range (not last−minute or over a year away)?\n",
        "                                C) Does the request avoid high−risk or restricted activities (e.g., extreme sports, off−grid travel)?\n",
        "                                D) Is there any mention of a valid passport or travel documentation?\n",
        "                                E) Is there enough information to follow up with a proposed itinerary?\"\"\"\n",
        "                            }\n",
        "                        ],\n",
        "                        'messages': [{\n",
        "                            'role': 'user',\n",
        "                            'content': [\n",
        "                                {\n",
        "                                    'text': \"Trip request: {{event_request}}\"\n",
        "                                }\n",
        "                            ]\n",
        "                        }],\n",
        "                        'inputVariables' : [\n",
        "                            { 'name' : 'event_request'}\n",
        "                        ]\n",
        "                    }\n",
        "                }\n",
        "        }]\n",
        "    )\n",
        "    print(\"Created!\")\n",
        "    prompt_arn = response.get(\"arn\")\n",
        "except bedrock.exceptions.ConflictException as e:\n",
        "    print(\"Already exists!\")\n",
        "    response = bedrock.list_prompts()\n",
        "    prompt = next((prompt for prompt in response['promptSummaries'] if prompt['name'] == \"TripBooker_xyz\"), None)\n",
        "    prompt_arn = prompt['arn']\n",
        "\n",
        "prompt_arn"
      ],
      "metadata": {
        "id": "FX5Nwj2f8WAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output should look similar to this: `Created! arn:aws:bedrock:us-east-1:12345678910:prompt/5E0L1VBVM1`"
      ],
      "metadata": {
        "id": "XkXubZ8o8XRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = bedrock.converse(\n",
        "    modelId=prompt_arn,\n",
        "    promptVariables={\n",
        "        'event_request': {\n",
        "            'text': \"\"\"\n",
        "                Hi there! I'm planning a trip to Italy with my partner and would love some help organizing the itinerary. We're hoping to travel between September 10–20 this year, ideally flying into Rome and spending a few days in Florence and Venice as well. We’d love recommendations on tours, cultural sites, and good local restaurants. We’re not interested in anything risky like skydiving or hiking remote trails — just want a relaxing and enriching experience. We both have valid passports. Let me know what other details you need!\n",
        "                \"\"\"\n",
        "        }\n",
        "    },\n",
        ")\n",
        "print(response['output']['message']['content'][0]['text'])"
      ],
      "metadata": {
        "id": "_UNKeIfy8eRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an AWS Lambda function\n",
        "\n",
        "Bedrock and Lambda are two tools that you can pair together that compliment each other well. While LLMs are very powerful, they are unable to do everything you need them to do. Bedrock is able to reach out to Lambda to call functions on your behalf. This allows you to \"build\" in new features to Bedrock.\n",
        "\n",
        "In this task, you'll be creating a Lambda function that performs mathematical operations. In a future task, this will be utilized by Bedrock.\n",
        "\n"
      ],
      "metadata": {
        "id": "5unwEq938kDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def add(num1, num2):\n",
        "    return num1 + num2\n",
        "\n",
        "def subtract(num1, num2):\n",
        "    return num1 − num2\n",
        "\n",
        "def multiply(num1, num2):\n",
        "    return num1 * num2\n",
        "\n",
        "def divide(num1, num2):\n",
        "    if num2 == 0:\n",
        "        raise ValueError(\"division by zero\")\n",
        "    return num1 / num2\n",
        "\n",
        "def power(num1, num2):\n",
        "    return num1 ** num2\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    operation = event.get('operation')\n",
        "    try:\n",
        "        num1 = float(event['num1'])\n",
        "        num2 = float(event['num2'])\n",
        "    except (KeyError, ValueError):\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps({'error': 'Invalid input, num1 and num2 must be numeric'})\n",
        "        }\n",
        "\n",
        "    # Map operations to functions\n",
        "    operations = {\n",
        "        'add': add,\n",
        "        'subtract': subtract,\n",
        "        'multiply': multiply,\n",
        "        'divide': divide,\n",
        "        'power': power\n",
        "    }\n",
        "\n",
        "    # Find the operation function and execute\n",
        "    operation_func = operations.get(operation)\n",
        "\n",
        "    if operation_func:\n",
        "        try:\n",
        "            result = operation_func(num1, num2)\n",
        "            return {\n",
        "                'statusCode': 200,\n",
        "                'body': json.dumps({'result': result})\n",
        "            }\n",
        "        except ValueError as e:\n",
        "            return {\n",
        "                'statusCode': 400,\n",
        "                'body': json.dumps({'error': str(e)})\n",
        "            }\n",
        "    else:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps({'error': 'invalid operation'})\n",
        "        }"
      ],
      "metadata": {
        "id": "9MrDfiVF8nZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Bedrock and Lambda"
      ],
      "metadata": {
        "id": "Idj7mBd6814o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Bedrock and Lambda\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "bedrock = boto3.client(service_name='bedrock-runtime')\n",
        "lambda_client = boto3.client(\"lambda\")\n",
        "MODEL_ID = \"amazon.nova-micro-v1:0\"\n",
        "\n",
        "# Define the calculation tool\n",
        "math_tool = {\n",
        "    \"toolSpec\": {\n",
        "        \"name\": \"calculateNumbers\",\n",
        "        \"description\": \"Performs basic arithmetic operations\",\n",
        "        \"inputSchema\": {\n",
        "            \"json\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"operation\": {\"type\": \"string\"},\n",
        "                    \"num1\": {\"type\": \"number\"},\n",
        "                    \"num2\": {\"type\": \"number\"}\n",
        "                },\n",
        "                \"required\": [\"operation\", \"num1\", \"num2\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to trigger the Lambda calculation service\n",
        "def execute_calculation(input_data):\n",
        "    response = lambda_client.invoke(\n",
        "        FunctionName=\"math-function\",\n",
        "        InvocationType=\"RequestResponse\",\n",
        "        Payload=json.dumps(input_data)\n",
        "    )\n",
        "    response_payload = response[\"Payload\"].read()\n",
        "    calculation_result = json.loads(response_payload)\n",
        "    response_body = calculation_result.get(\"body\", \"{}\")\n",
        "    return json.loads(response_body) if isinstance(response_body, str) else response_body\n",
        "\n",
        "# User's initial message\n",
        "user_input = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"text\": \"Please subtract 60 from 100\"}]\n",
        "}\n",
        "\n",
        "# Define system instructions\n",
        "system_instructions = [\n",
        "    {\"text\": \"\"\"\n",
        "    You are a virtual assistant capable of performing basic arithmetic operations: add, subtract, multiply, and divide.\n",
        "    If the user doesn't specify an operation, ask them for more details.\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# First interaction with the model\n",
        "first_interaction = bedrock.converse(\n",
        "    modelId=MODEL_ID,\n",
        "    system=system_instructions,\n",
        "    messages=[user_input],\n",
        "    toolConfig={\n",
        "        \"tools\": [math_tool],\n",
        "        \"toolChoice\": {\"auto\": {}}\n",
        "    },\n",
        "    inferenceConfig={\"temperature\": 0.7}\n",
        ")\n",
        "\n",
        "# Process the assistant's response to check if tool is required\n",
        "assistant_reply = first_interaction[\"output\"][\"message\"]\n",
        "message_parts = assistant_reply[\"content\"]\n",
        "tool_request_block = next((part for part in message_parts if \"toolUse\" in part), None)\n",
        "\n",
        "if not tool_request_block:\n",
        "    print(\"=== Assistant's Direct Response ===\")\n",
        "    print(message_parts[0][\"text\"])\n",
        "else:\n",
        "    tool_request = tool_request_block[\"toolUse\"]\n",
        "    tool_input_data = tool_request[\"input\"]\n",
        "    tool_id = tool_request[\"toolUseId\"]\n",
        "    print(tool_request_block)\n",
        "    print(f\"→ Assistant triggered tool: calculateNumbers with input: {tool_input_data}\")\n",
        "\n",
        "    # Execute the requested tool\n",
        "    tool_result = execute_calculation(tool_input_data)\n",
        "    print(f\"← Lambda Function output: {tool_result}\")\n",
        "\n",
        "    # Create a response based on the tool's output\n",
        "    try:\n",
        "        result_summary = f\"The outcome of the calculation is {tool_result['result']}.\"\n",
        "    except Exception as e:\n",
        "        result_summary = f\"Oops! There was an error with the calculation. ({str(e)})\"\n",
        "\n",
        "    # Generate tool result message\n",
        "    tool_response_msg = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"toolResult\": {\n",
        "                    \"toolUseId\": tool_id,\n",
        "                    \"content\": [{\"text\": result_summary}]\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Send tool result back to the model\n",
        "    final_output = bedrock.converse(\n",
        "        modelId=MODEL_ID,\n",
        "        messages=[user_input, assistant_reply, tool_response_msg],\n",
        "        toolConfig={\n",
        "            \"tools\": [math_tool],\n",
        "            \"toolChoice\": {\"auto\": {}}\n",
        "        },\n",
        "        inferenceConfig={\"temperature\": 0.7}\n",
        "    )\n",
        "\n",
        "    # Display the final response from the assistant\n",
        "    final_message = final_output[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "    print(\"\\n=== Final Assistant Response ===\")\n",
        "    print(final_message)"
      ],
      "metadata": {
        "id": "fG8uDvQa86td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output should look similar to this example:\n",
        "\n",
        "```\n",
        "{'toolUse': {'toolUseId': 'tooluse_mCTQPfNcTvGmiclwGbD2bA', 'name': 'calculateNumbers', 'input': {'num1': 100, 'operation': 'subtract', 'num2': 60}}}\n",
        "Assistant triggered tool: calculateNumbers with input: {'num1': 100, 'operation': 'subtract', 'num2': 60}\n",
        "Lambda Function output: {'result': 40.0}\n",
        "\n",
        "=== Final Assistant Response ===\n",
        "The outcome of subtracting 60 from 100 is 40.0.\n",
        "```"
      ],
      "metadata": {
        "id": "jTNi13XJ9A6W"
      }
    }
  ]
}
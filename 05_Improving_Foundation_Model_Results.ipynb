{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrPbQ2JpDO1wDo3AXH/FMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suplab/amazon-bedrock-genai-labs/blob/main/05_Improving_Foundation_Model_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Amazon Bedrock"
      ],
      "metadata": {
        "id": "XhZaoVnXNc2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore how the LangChain framework streamlines the development of large language model (LLM) applications, particularly when integrated with Amazon Bedrock.\n",
        "\n",
        "LangChain provides a structured interface that simplifies common tasks such as prompt design, state management, and output formatting—key challenges in building production-grade AI tools.\n",
        "\n",
        "Through components like ChatBedrock, reusable prompt templates, and Pydantic-based output parsers, LangChain enables developers to create maintainable, scalable, and type-safe LLM workflows.\n",
        "\n",
        "By chaining operations and abstracting complex logic, the framework helps reduce boilerplate code while increasing modularity and reliability in generative AI applications."
      ],
      "metadata": {
        "id": "1oImAVM5NokJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Framework\n",
        "\n",
        "* **Definition:** A framework providing a standard interface for interacting with LLMs\n",
        "* **Primary Purpose:** Simplifies building production-ready LLM applications\n",
        "* **Integration:** Works with Amazon Bedrock and other LLM platforms"
      ],
      "metadata": {
        "id": "Xa3A4pBO3WZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatBedrock Implementation\n",
        "\n",
        "```python\n",
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "chat = ChatBedrock(\n",
        "    model_id=\"...\",\n",
        "    temperature=0\n",
        ")\n",
        "```\n",
        "\n",
        "* Provides chat-style interface for LLM interaction\n",
        "* Uses similar parameters as direct Bedrock APIs"
      ],
      "metadata": {
        "id": "LNGcReMy3oJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Message Structure\n",
        "\n",
        "* System Messages: Define assistant behavior/role\n",
        "* Human Messages: User inputs\n"
      ],
      "metadata": {
        "id": "N3J8LN_g3zuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "    content=\"You are a helpful assistant! Your name is Bob.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "    content=\"What is your name?\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Define a chat model and invoke it with the messages\n",
        "print(model.invoke(messages))"
      ],
      "metadata": {
        "id": "gsEfIdAm4dTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "\n",
        "* Allows creation of reusable prompts with placeholders\n",
        "* Can chain operations together using pipe operator\n",
        "\n"
      ],
      "metadata": {
        "id": "39HgLbpV4HU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuopLfkhNOqJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "    \"name\": \"Bob\",\n",
        "    \"user_input\": \"What is your name?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing a Model with Amazon Bedrock"
      ],
      "metadata": {
        "id": "U0NA0Sne4p8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets customize foundation models in Amazon Bedrock using three advanced techniques: **fine-tuning**, **continuous pre-training**, and **model distillation**. These approaches allow you to tailor model behavior, improve domain-specific understanding, and optimize performance for production use.\n",
        "\n",
        "- **Fine-tuning** uses labeled input-output pairs to guide model responses toward specific formats or tones,\n",
        "\n",
        "- while **continuous pre-training** strengthens a model’s familiarity with specialized domains by leveraging large volumes of unlabeled data.\n",
        "\n",
        "- **Model distillation** helps create smaller, more efficient models by transferring knowledge from a larger, more capable one. Understanding when and how to apply each method is key to building scalable, accurate, and cost-effective AI solutions in Amazon Bedrock."
      ],
      "metadata": {
        "id": "U4QwxRzd6DTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Pre-training\n",
        "\n",
        "- **Purpose:** Improves model's domain understanding using large amounts of unlabeled data\n",
        "\n",
        "- **Key characteristics:**\n",
        "\n",
        "  * Uses raw domain-specific data rather than labeled examples\n",
        "  * Requires large-scale, high-quality datasets\n",
        "  * Helps with specialized fields (healthcare, finance, manufacturing)\n",
        "  * Can be updated as new data becomes available"
      ],
      "metadata": {
        "id": "vb_75C9d6Y73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Distillation\n",
        "\n",
        "- **Purpose:** Creates smaller, more efficient models while maintaining accuracy\n",
        "\n",
        "- **Components:**\n",
        "\n",
        "  * Teacher model: Larger, more capable model (e.g., Nova Pro)\n",
        "  * Student model: Smaller, faster model (e.g., Nova Lite)\n",
        "  * Minimum 100 prompts needed\n",
        "\n",
        "- **Benefits:**\n",
        "\n",
        "  * Reduced latency\n",
        "  * Better cost efficiency\n",
        "  * Knowledge transfer from larger to smaller model"
      ],
      "metadata": {
        "id": "_i-vvnDO6tTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customization Approaches"
      ],
      "metadata": {
        "id": "j98ugDuZ7Ltm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "* Fastest, flexible approach\n",
        "* Uses system prompts, few-shot examples, and formatting\n",
        "* No custom training required\n",
        "* Limitations: Token consumption, consistency issues\n",
        "\n"
      ],
      "metadata": {
        "id": "GpWfzwok7O6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Augmented Generation (RAG)\n",
        "- Connects model to external knowledge bases\n",
        "- Dynamically retrieves relevant content\n",
        "- Good for factual accuracy and current information\n",
        "- Example: Using S3-stored documents as knowledge base"
      ],
      "metadata": {
        "id": "Z5xaFoLz7eE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning\n",
        "- Uses labeled datasets (input-output pairs)\n",
        "- Shapes model behavior for specific tasks\n",
        "- Requires provisioned throughput\n",
        "- Best for:\n",
        "  * Consistent tone/format\n",
        "  * Understanding domain-specific input\n",
        "  * Structured outputs"
      ],
      "metadata": {
        "id": "QFO3bd8x7wcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Considerations:\n",
        "\n",
        "* **Data preparation:**\n",
        "\n",
        "  - Filtering irrelevant data\n",
        "  - Pre-processing for quality\n",
        "  - Storage in Amazon S3\n",
        "\n",
        "* **Model deployment:**\n",
        "\n",
        "  - Testing in dev environment\n",
        "  - Monitoring performance\n",
        "  - Managing context windows\n",
        "  - Handling prompt history"
      ],
      "metadata": {
        "id": "lq1JwmxY7-5D"
      }
    }
  ]
}
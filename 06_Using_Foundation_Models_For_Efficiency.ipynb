{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSfS3uJvovvcCloM/fPzsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suplab/amazon-bedrock-genai-labs/blob/main/06_Using_Foundation_Models_For_Efficiency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Assessment and Efficiency"
      ],
      "metadata": {
        "id": "zmR4cLjpE7-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the core mechanisms Amazon Bedrock provides for evaluating and optimizing AI model performance at scale. The material introduces several evaluation strategies including programmatic, model-based, human-led, and RAG-specific assessments.\n",
        "\n",
        "Beyond evaluation, we highlights techniques for operational efficiency, such as prompt caching, intelligent prompt routing, and cross-region inference profiles.\n",
        "\n",
        "These capabilities enable developers to streamline model responses, reduce latency, cut costs, and dynamically allocate resources, making Amazon Bedrock a powerful platform for scalable, high-performance AI deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKNC-iiXE_62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bedrock Evaluations\n",
        "\n",
        "### Automatic/Programmatic Evaluation\n",
        "- Tests metrics like toxicity, accuracy, and robustness\n",
        "- Configurable parameters: temperature, top P, response length\n",
        "- Supports different task types (text generation, summarization, Q&A, classification)\n",
        "\n",
        "### Judge Model Evaluation\n",
        "- Uses an evaluator model to generate metrics\n",
        "- Metrics include quality, helpfulness, relevance, coherence, responsible AI\n",
        "- Can evaluate external inferences via JSONL files\n",
        "\n",
        "### Human Worker Evaluation\n",
        "- Uses human evaluators instead of models\n",
        "- Can compare responses from two models\n",
        "- Supports rating methods like thumbs up/down or Likert scale\n",
        "\n",
        "### RAG Evaluation\n",
        "- Evaluates retrieval-augmented generation systems\n",
        "- Two evaluation types: Retrieval + Response Generation, Retrieval Only\n",
        "- Metrics vary by evaluation type\n",
        "\n",
        "### Prompt Caching\n",
        "- Purpose: Optimize token usage and response time for repeated prompt components\n",
        "- Benefits: Reduces token consumption Improves response speed Lower costs\n",
        "- Limitations: Requires exact match of prefix content Sensitive to formatting changes\n",
        "- Implementation Example:\n",
        "```python\n",
        " cache_point = {\"type\": \"default\"}\n",
        "  prompt = {\n",
        "    \"system_prompt\": \"...\",\n",
        "    \"cache_point\": cache_point,\n",
        "    \"user_input\": \"...\"\n",
        "  }\n",
        "```\n",
        "\n",
        "### Intelligent Prompt Routing\n",
        "- Purpose: Automatically direct requests to appropriate models\n",
        "- Features:\n",
        "  * Dynamic model selection based on request complexity\n",
        "  * Cost optimization:\n",
        "  * Supports model families (e.g., Meta, Anthropic)\n",
        "\n",
        "- Implementation Example:\n",
        "```python\n",
        " routing_config = {\n",
        "    \"models\": [\"<MODEL_ID_1>\", \"<MODEL_ID_2>\"],\n",
        "    \"routing_criteria\": {\"response_quality_difference\": <DIFFERENCE>},\n",
        "    \"fallback_model\": \"<FALLBACK_MODEL_ID>\"\n",
        "  }\n",
        "```\n",
        "\n",
        "### Cross-Region Inference Profiles\n",
        "- Purpose: Optimize model execution across regions\n",
        "- Benefits:\n",
        "  * Improved latency\n",
        "  * Better availability\n",
        "  * Load balancing\n",
        "- Can be combined with prompt routing for comprehensive optimization"
      ],
      "metadata": {
        "id": "6AjPXBZPFEl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Caching with Amazon Bedrock"
      ],
      "metadata": {
        "id": "mVE6rP1Q8rHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the Amazon Bedrock Converse API through an interactive chat session. You will learn how to cache responses and conserve tokens to reduce the overall cost of using Bedrock."
      ],
      "metadata": {
        "id": "2nJngyrZ9Isd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Bedrock"
      ],
      "metadata": {
        "id": "Rcbd2at89OCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w82JTUp79Siq",
        "outputId": "68d44f39-1b57-448a-f5f4-379818dd1f53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.42.33-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<1.43.0,>=1.42.33 (from boto3)\n",
            "  Downloading botocore-1.42.33-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.33->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.33->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.33->boto3) (1.17.0)\n",
            "Downloading boto3-1.42.33-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.33-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.42.33 botocore-1.42.33 jmespath-1.1.0 s3transfer-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sHUvvjWH8hVn"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
        "\n",
        "MODEL_ID = \"amazon.nova-micro-v1:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bedrock"
      ],
      "metadata": {
        "id": "vhSkGSDB9Y2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = '''\n",
        " You are an assistant that summarizes music reviews for a record company.\n",
        " Here are examples:\n",
        "\n",
        " Review: The latest album by The New Wave Band is a masterpiece! Every track is a hit.\n",
        " Summary: Reviewer praises the latest album as a masterpiece with hit tracks.\n",
        "\n",
        " Review: I was disappointed with the new single; it lacked the energy of their previous work.\n",
        " Summary: Reviewer expresses disappointment, noting a lack of energy compared to previous work.\n",
        " '''\n",
        "\n",
        "user_input_review = \"This EP is a solid effort with a few standout songs, though some tracks feel repetitive.\""
      ],
      "metadata": {
        "id": "zHvGaoXB9egT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will tell Bedrock what it's going to be doing for you. It provides examples of customer reviews, and then sets it up with one that you'll be using in an upcoming step."
      ],
      "metadata": {
        "id": "8UrqeyL39k5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No caching"
      ],
      "metadata": {
        "id": "GkuHndseBzIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_cache_payload = {\n",
        "     \"system\": [\n",
        "         {\"text\": system_prompt}\n",
        "     ],\n",
        "     \"messages\": [\n",
        "         {\n",
        "             \"role\": \"user\",\n",
        "             \"content\": [\n",
        "                 {\"text\": user_input_review + \"\\nSummary:\"}\n",
        "             ]\n",
        "         }\n",
        "     ]\n",
        " }\n",
        "\n",
        "no_cache_response = bedrock.converse(\n",
        "     modelId=MODEL_ID,\n",
        "     system=no_cache_payload[\"system\"],\n",
        "     messages=no_cache_payload[\"messages\"]\n",
        " )\n",
        "\n",
        "no_cache_output = no_cache_response['output']['message']['content'][0]['text']\n",
        "no_cache_input_tokens = no_cache_response['usage']['inputTokens']\n",
        "no_cache_output_tokens = no_cache_response['usage']['outputTokens']\n",
        "no_cache_tokens = no_cache_input_tokens + no_cache_output_tokens\n",
        "\n",
        "print(\"[No Caching] Generated Summary:\")\n",
        "print(no_cache_output)\n",
        "print(f\"Total Tokens Used (No Cache): {no_cache_tokens}\")"
      ],
      "metadata": {
        "id": "R720QgjBB5pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response should look similar to the following:\n",
        "\n",
        "```\n",
        "[No Caching] Generated Summary:\n",
        "Reviewer acknowledges the EP as a solid effort with a few standout songs, but notes that some tracks feel repetitive.\n",
        "Total Tokens Used (No Cache): 130\n",
        "```"
      ],
      "metadata": {
        "id": "-VywHepLCQZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With caching"
      ],
      "metadata": {
        "id": "1Fz3te6RCYtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_point = {\"cachePoint\": {\"type\": \"default\"}}\n",
        "\n",
        "payload_with_caching = {\n",
        "     \"system\": [\n",
        "         {\"text\": system_prompt},\n",
        "         cache_point\n",
        "     ],\n",
        "     \"messages\": [\n",
        "         {\n",
        "             \"role\": \"user\",\n",
        "             \"content\": [\n",
        "                 {\"text\": user_input_review + \"\\nSummary:\"}\n",
        "             ]\n",
        "         }\n",
        "     ]\n",
        " }\n",
        "\n",
        "response = bedrock.converse(\n",
        "     modelId=MODEL_ID,\n",
        "     system=payload_with_caching[\"system\"],\n",
        "     messages=payload_with_caching[\"messages\"]\n",
        " )\n",
        "\n",
        "cache_output = response['output']['message']['content'][0]['text']\n",
        "cache_input_tokens = response['usage']['inputTokens']\n",
        "cache_output_tokens = response['usage']['outputTokens']\n",
        "cache_tokens = cache_input_tokens + cache_output_tokens\n",
        "\n",
        "print(\"[With Caching] Generated Summary:\")\n",
        "print(cache_output)\n",
        "print(f\"Total Tokens Used: {cache_tokens}\")"
      ],
      "metadata": {
        "id": "Gl_EjqHCCjJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response should look similar to the following:\n",
        "\n",
        "```\n",
        " [With Caching] Generated Summary:\n",
        " Reviewer acknowledges the EP as a solid effort with a few standout songs, but notes that some tracks feel repetitive.\n",
        " Total Tokens Used: 46\n",
        "```\n",
        "Notice the dramatic decrease in token usage? This is because Bedrock has processed this exact request before and can return the same response without using as many tokens as the initial processing required."
      ],
      "metadata": {
        "id": "ob43dkcBCq5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing your Bedrock prompt"
      ],
      "metadata": {
        "id": "qU5R9RFOC6a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        " You are a pet expert that will tell the user what type of pet they have based on a description. You'll keep your responses short and generalized.\n",
        " Here are examples:\n",
        "\n",
        " Description: My pet barks and has four legs.\n",
        " Summary: Your pet is a dog.\n",
        "\n",
        " Review: My pet meows and uses a litter box.\n",
        " Summary: Your pet is a cat.\n",
        " \"\"\"\n",
        "\n",
        "user_input_review = \"My pet sings and has wings.\""
      ],
      "metadata": {
        "id": "BNLKppXnC-Bj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell `With Caching`, the response should look similar to this:\n",
        "\n",
        "```\n",
        "[With Caching] Generated Summary:\n",
        "Your pet is likely a bird, such as a parrot or canary.\n",
        "Total Tokens Used: 26\n",
        " ```\n",
        "\n",
        "Run the cell `No Caching`, the response should look similar to this:\n",
        "\n",
        "```\n",
        "[No Caching] Generated Summary:\n",
        " Your pet is likely a bird, such as a parrot or canary.\n",
        " Total Tokens Used (No Cache): 98\n",
        " ```\n",
        "\n",
        " Why does the cached response use less tokens even with a new request? Using `cache_point = {\"cachePoint\": {\"type\": \"default\"}}` with Bedrock on a new request can still use fewer tokens because the system leverages pre-cached computations and optimizations. Even for new requests, parts of the model's processing might be reused from the cache, reducing the need for fresh token computation. This caching strategy helps minimize redundant processing, leading to lower token usage and cost. Essentially, Bedrock efficiently reuses cached intermediate results, even on the first request."
      ],
      "metadata": {
        "id": "y1vSa9iNDOd5"
      }
    }
  ]
}